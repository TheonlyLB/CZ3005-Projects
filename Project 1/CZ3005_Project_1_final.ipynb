{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cbf929e8",
      "metadata": {
        "id": "cbf929e8"
      },
      "source": [
        "# CZ3005 Project 1 - Balancing a Pole on a Cart"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e6fb8e",
      "metadata": {
        "id": "34e6fb8e"
      },
      "source": [
        "### Done by: Zon Liew (U1921098F), Charlotte Teo (U2022021G), Paul Low (U2022421F)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa7c32e",
      "metadata": {
        "id": "0fa7c32e"
      },
      "source": [
        "Objectives:\n",
        "Apply Linear annealed policy with the EpsGreedyQPolicy as the inner policy:\n",
        "Achieve a DQN model that trains in the least possible number of episodes.\n",
        "Balance pole on the cart for 500 steps for 100 consecutive episodes while testing.\n",
        "\n",
        "Epsilon-Greedy chooses the optimal action at each step, but sometimes randomly chooses an unlikely option.\n",
        "We specify an initially high exploration rate (epsilon) of 1 at the beginning of Q function training because we know nothing about the importance of the Q table. Epsilon value is decreased as the agent has more confidence in the Q values.\n",
        "\n",
        "A DQN agent can be used in any environment which has a discrete action space.\n",
        "It is based on the Q - Network, a neural network model that can learn to predict Q-Values (expected returns) for all actions, given an observation from the environment.\n",
        "\n",
        "The hyperparameters are:\n",
        "\n",
        "Size of 1st fully connected layer: 256\n",
        "Size of 2nd fully connected layer: 512\n",
        "Period of the update of the target network parameters: 1000 steps\n",
        "Discount factor: 0.99\n",
        "Decay factor for epsilon in epsilon-greedy policy: 0.99\n",
        "Minimum epsilon in epsilon-greeddy policy: 1E-4\n",
        "Learning rate: 3E-4\n",
        "Size of replay memory: 1000000\n",
        "Period of experience replay: 4 steps\n",
        "PER alpha: 0.2\n",
        "PER beta0: 0.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee149986",
      "metadata": {
        "id": "ee149986"
      },
      "source": [
        "\n",
        "## Task 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4af0a72",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4af0a72",
        "outputId": "732de184-f132-452d-bcca-1325e7f63fb6"
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install gym[classic_control]\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VGCywaOX6Dv9",
      "metadata": {
        "id": "VGCywaOX6Dv9"
      },
      "source": [
        "# Import Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab9d7358",
      "metadata": {
        "id": "ab9d7358"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import RecordVideo\n",
        "gymlogger.set_level(40) #error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "import matplotlib.pyplot as plt\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11567af",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Install keras rl2 which seamlessly integrates with the  OpenAI Gym  to evaluate and play around with DQN Algorithm\n",
        "!pip install keras-rl2\n",
        "!pip install dopamine-rl\n",
        "#Install Open AI Gym for the Cart Pole Environment\n",
        "!pip install tensorflow --upgrade\n",
        "!pip install rl-agents==0.1.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d274e11a",
      "metadata": {},
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "import numpy as np\n",
        "import rl\n",
        "from rl.memory import SequentialMemory\n",
        "from rl.agents.dqn import DQNAgent\n",
        "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qJKIP-tc53rz",
      "metadata": {
        "id": "qJKIP-tc53rz"
      },
      "source": [
        "# Initialisation of Cartpole Environment\n",
        "Loaded from Open AI gym suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a989c1ca",
      "metadata": {
        "id": "a989c1ca"
      },
      "outputs": [],
      "source": [
        "#Load the CartPole environment from the OpenAI Gym suite\n",
        "env = gym.make(\"CartPole-v1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595026ba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "595026ba",
        "outputId": "880a2325-ef01-4e9d-b159-3542144a6202"
      },
      "outputs": [],
      "source": [
        "#Resets the environment to an initial state and returns the initial observation.\n",
        "initial_observation = env.reset()\n",
        "print(\"Initial observation:\", initial_observation)\n",
        "cumulative_reward = 0\n",
        "done = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "X8-aDhiR5JMQ",
      "metadata": {
        "id": "X8-aDhiR5JMQ"
      },
      "source": [
        "Observation corresponds to  'cart position', 'cart velocity', 'pole angle', and 'pole velocity' respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WpxvyDAD5xwQ",
      "metadata": {
        "id": "WpxvyDAD5xwQ"
      },
      "source": [
        "# Initialisation of Agent\n",
        "\n",
        "Setup of Deep Q-Network (DQN) agent using Keras-RL library for reinforcement learning.\n",
        "\n",
        "Important functions used include:\n",
        "1. SequentialMemory: This sets up an experience replay buffer with a capacity limit of 50,000 and a window length of 1. The buffer stores the agent's experiences so that they can be randomly sampled during the training process.\n",
        "2. LinearAnnealedPolicy: This sets up the exploration policy for the agent. It uses the EpsGreedyQPolicy as the inner policy which selects actions based on a trade-off between exploration and exploitation. The LinearAnnealedPolicy gradually decreases the exploration rate (eps) from 1.0 to 0.1 over 10,000 steps.\n",
        "3. Sequential: This sets up a feed-forward neural network model for the DQN. It has an input layer with a shape of (1, env.observation_space.shape[0]), which means it takes in a single observation vector of length env.observation_space.shape[0]. The hidden layers have 256 and 128 nodes respectively and use the ReLU activation function. The output layer has a number of nodes equal to the number of actions in the action space, and uses the linear activation function.\n",
        "4. DQNAgent: This sets up the DQN agent using the previously defined model, memory, policy, and other hyperparameters such as the number of warmup steps and the target model update rate. The enable_dueling_network argument is set to True, which means the agent will use a dueling architecture to estimate the Q-values of each action.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f6b018e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f6b018e",
        "outputId": "0aa49099-89e1-47ab-b564-23d5487f7996"
      },
      "outputs": [],
      "source": [
        "#Building DQN Agent with Keras-RL\n",
        "# setup experience replay buffer\n",
        "memory = SequentialMemory(limit=50000, window_length=1)\n",
        "\n",
        "# setup the Linear annealed policy with the EpsGreedyQPolicy as the inner policy\n",
        "policy =  LinearAnnealedPolicy(inner_policy=  EpsGreedyQPolicy(),   # policy used to select actions\n",
        "                               attr='eps',                          # attribute in the inner policy to vary             \n",
        "                               value_max=1.0,                       # maximum value of attribute that is varying\n",
        "                               value_min=0.1,                       # minimum value of attribute that is varying\n",
        "                               value_test=0.05,                     # test if the value selected is < 0.05\n",
        "                               nb_steps=10000)                      # the number of steps between value_max and value_min\n",
        "\n",
        "#Feed-Forward Neural Network Model for Deep Q Learning (DQN)\n",
        "model = Sequential()\n",
        "#print(env.observation_space)\n",
        "#Input is 1 observation vector, and the number of observations in that vector \n",
        "model.add(Input(shape=(1,env.observation_space.shape[0])))\n",
        "model.add(Flatten())\n",
        "#Hidden layers with 24 nodes each\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "#Output is the number of actions in the action space\n",
        "model.add(Dense(env.action_space.n, activation='linear')) \n",
        "\n",
        "\n",
        "#Feed-Forward Neural Network Architecture Summary\n",
        "print(model.summary())\n",
        "\n",
        "#Defining DQN Agent for DQN Model\n",
        "dqn = DQNAgent(model=model,                     # Q-Network model\n",
        "               nb_actions=env.action_space.n,   # number of actions\n",
        "               memory=memory,                   # experience replay memory\n",
        "               nb_steps_warmup=25,              # how many steps are waited before starting experience replay\n",
        "               target_model_update=1e-2,        # how often the target network is updated\n",
        "               policy=policy,                   # the action selection policy\n",
        "              enable_dueling_network=True)                   \n",
        "\n",
        "# Configure and compile agent. \n",
        "#Use built-in tensorflow.keras Adam optimizer and evaluation metrics            \n",
        "#Adam._name = 'Adam'\n",
        "dqn.compile(keras.optimizers.Adam(learning_rate=2.5e-4,epsilon = 0.01), metrics = [\"mse\",'accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "     "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qCvABuzd9i15",
      "metadata": {
        "id": "qCvABuzd9i15"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "b0af5288",
      "metadata": {},
      "source": [
        "# Training "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2ff3b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a2ff3b9",
        "outputId": "144e4397-fb9b-4142-a12c-1c64f432b412"
      },
      "outputs": [],
      "source": [
        "#Finally fit and train the agent\n",
        "#Verbose parameter controls how much information is printed during training. A value of 10 means that training progress is printed every 10 steps.\n",
        "history = dqn.fit(env, nb_steps=5000, visualize=False, verbose=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "306acdfd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "id": "306acdfd",
        "outputId": "e6f16be0-af4d-4c5c-9a60-d87377752ac0"
      },
      "outputs": [],
      "source": [
        "# Visualize the history for number of Training episode steps of the Cart Pole Game\n",
        "plt.figure(figsize = (18,10))\n",
        "plt.plot(history.history['nb_episode_steps'])\n",
        "plt.ylabel('nb_episode_steps')\n",
        "plt.xlabel('episodes')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30de5f0",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22856561",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22856561",
        "outputId": "757a1620-2816-4652-c867-10efc345e535"
      },
      "outputs": [],
      "source": [
        "# Finally, evaluate and test our algorithm for 100 episodes.\n",
        "dqn.test(env, nb_episodes=100, visualize=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca311154",
      "metadata": {
        "id": "ca311154"
      },
      "outputs": [],
      "source": [
        "# After training is done, we save the final weights.\n",
        "dqn.save_weights('dqn_weights.h5f', overwrite=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c05f5658",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c05f5658",
        "outputId": "f41eaa9d-e45a-4016-f429-45dcbb2d330f"
      },
      "outputs": [],
      "source": [
        "observation = env.reset()\n",
        "dqn.load_weights('dqn_weights.h5f')\n",
        "action = dqn.forward(observation)\n",
        "\n",
        "print(\"Observation: \", observation)\n",
        "print(\"Chosen action: \", action)\n",
        "\n",
        "new_observation, reward, done, info = env.step(action)\n",
        "print(\"Observations after action: \", new_observation)\n",
        "print(\"Reward for this step: \", reward)\n",
        "print(\"Episode Completion: \", done)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8a1c678",
      "metadata": {
        "id": "b8a1c678"
      },
      "source": [
        "## Task 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae724e8",
      "metadata": {
        "id": "eae724e8"
      },
      "source": [
        "Each Episode comprises of multiple steps. The environment is reset at the start of each episode as required. Next, the Agent makes the forward step based on the observation, before updating the new episode reward and observation for the next step. This continues until the termination criteria or truncation value of 500. Each cumulative reward is stored in a list and mapped out in the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8277289",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8277289",
        "outputId": "c82f2064-3657-445f-a77b-04b3399e36c5"
      },
      "outputs": [],
      "source": [
        "num_episodes = 100\n",
        "episode_results = []\n",
        "dqn.load_weights('dqn_weights.h5f')\n",
        "for i in range(1,num_episodes+1):\n",
        "    # Reset environment at the beginning of each episode\n",
        "    observation = env.reset()\n",
        "    cumulative_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Agent takes action based on observation\n",
        "        action = dqn.forward(observation)\n",
        "        \n",
        "        # Environment processes action and returns new observation, reward, and done flag\n",
        "        new_observation, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update episode reward and observation for next step\n",
        "        observation = new_observation\n",
        "        cumulative_reward += reward\n",
        "        \n",
        "    print(\"Episode:\", i, \" Cumulative reward: \", cumulative_reward)\n",
        "    episode_results.append(cumulative_reward)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47225d21",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "47225d21",
        "outputId": "4c8dc73f-f091-407e-f8a6-3e15384385e9"
      },
      "outputs": [],
      "source": [
        "plt.plot(episode_results)\n",
        "plt.title('Cumulative reward for each episode')\n",
        "plt.ylabel('Cumulative reward')\n",
        "plt.xlabel('episode')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdfebdd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdfebdd5",
        "outputId": "1dfcbd87-8af1-4cae-9bf4-014223884135"
      },
      "outputs": [],
      "source": [
        "mean = sum(episode_results) / len(episode_results)\n",
        "print(\"Average cumulative reward:\", mean)\n",
        "print(\"Is my agent good enough?\", mean > 195)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a401987",
      "metadata": {
        "id": "1a401987"
      },
      "source": [
        "## Task 3\n",
        "Render an episode played by the developed RL agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44bfaa04",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "44bfaa04",
        "outputId": "56934974-ffd3-4f3f-f429-3afc0cfea375"
      },
      "outputs": [],
      "source": [
        "env1 = RecordVideo(gym.make(\"CartPole-v1\"), \"./video\")\n",
        "observation = env1.reset()\n",
        "while True:\n",
        "    env1.render()\n",
        "    #your agent goes here\n",
        "    action = dqn.forward(observation)\n",
        "    observation, reward, done, info = env.step(action) \n",
        "    if done: \n",
        "      break;    \n",
        "env1.close()\n",
        "show_video()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ffd533",
      "metadata": {
        "id": "14ffd533"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "993f40e0",
      "metadata": {
        "id": "993f40e0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23588315",
      "metadata": {
        "id": "23588315"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b59a75b",
      "metadata": {
        "id": "8b59a75b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
